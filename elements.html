<!DOCTYPE HTML>

<html>
	<head>
		<title>Realtime Work scenarios</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
		<div id="wrapper">

			<!-- Header -->
			<header id="header">
				<a href="index.html" class="logo">Real time usecases</a>
			</header>

			<!-- Nav -->
			<nav id="nav">
				<ul class="links">
					<li><a href="generic.html">My Skills</a></li>
					<li><a href="index.html">Academic Projects</a></li>
					<li class="active"><a href="elements.html">Work Experience</a></li>
				</ul>
			</nav>

			<!-- Main -->
			<div id="main">

				<!-- Post -->
				<section class="post">
					<header class="major">

						<style>
							body {
								font-family: Arial, sans-serif;
								line-height: 1.6;
								margin: 0;
								padding: 0;
							}
							.container {
								max-width: 800px;
								margin: auto;
								padding: 20px;
								text-align: justify;
							}
							h2, h3, h4 {
								color: #333;
							}
							ul {
								list-style-type: none;
								padding: 0;
							}
							li {
								margin-bottom: 10px;
							}
						</style>

						<div class="container">
							<p><i>Here, we delve into compelling use cases that illustrate the power of automated data pipelines in driving actionable insights and informed decision-making.</i></p>
							
							<h3>Use Case 1:</h3>
							<p>ETL pipeline to extract data from various sources and loading for further analytics</p>
							
							<h4>Challenge:</h4>
							<p>Our organization faced the challenge of efficiently processing vast volumes of financial data sourced from different sources involving SharePoint, API, and SFTP.</p>
							
							<h4>Solution:</h4>
							<p>To address this challenge, we implemented a robust automated pipeline leveraging Amazon Web Services (AWS) infrastructure. The solution comprised multiple components orchestrated seamlessly to streamline the entire data processing workflow.</p>
							
							<ul>
								<li><strong>Data Extraction:</strong> We developed an automated pipeline to fetch financial data from the API and store it in Amazon S3 in JSON format. This step ensured a continuous flow of raw data into our ecosystem.</li>
								<li><strong>Data Transformation:</strong> Leveraging AWS Glue, we executed transformations on the raw JSON data, converting it into the highly efficient Parquet format. This optimization significantly enhanced storage efficiency while maintaining data integrity.</li>
								<li><strong>Data Loading:</strong> The transformed data was then loaded into Amazon Redshift, a powerful data warehouse solution, where it underwent advanced analytics processes. This step laid the foundation for deriving valuable insights and facilitating data-driven decisions.</li>
							</ul>
							
							<ul>
								<li>Integrated AWS Lambda for trigger-based data processing and validation.</li>
								<li>Used AWS Glue and Glue Crawlers to transform JSON data to Parquet format and automate schema discovery.</li>
								<li>Implemented Amazon SNS for real-time alerts and notifications.</li>
								<li>Leveraged AWS Step Functions for orchestrating the entire ETL workflow.</li>
								<li>Utilized Amazon CloudWatch for monitoring, logging, and ensuring pipeline performance.</li>
								<li>Ensured security with AWS IAM roles and policies for controlled access to services.</li>
							</ul>
						</div>

						<div class="container">
							<h3>Use Case 2:</h3>
							<p>Data Processing with AWS Glue and Athena</p>
							
							<p>For one of the projects, I contributed to the data processing pipeline which involved the collection and processing of chat data.</p>
							
							<ul>
								<li><strong>Data Collection:</strong> Chat data was collected and sent to AWS Kinesis using Snowplow.</li>
								<li><strong>Data Segregation:</strong> The data in AWS Kinesis was segregated based on events and pushed to different buckets in S3.</li>
								<li><strong>Data Transformation:</strong> Glue jobs were used to read the segregated data from S3, structure it, and load it into Athena.</li>
								<li><strong>Feature Development:</strong> The developer team added new features and additional columns to the data, which required monitoring and validation.</li>
								<li><strong>Validation Task:</strong> My task was to verify the added additional columns in Athena to ensure data consistency and integrity.</li>
								<li><strong>ETL Job Configuration:</strong> For the Glue jobs, I configured the source as S3 and the destination as Athena, specifying the table name and database name.</li>
								<li><strong>Job Scheduling:</strong> Airflow was utilized for scheduling ETL jobs, ensuring timely execution and monitoring.</li>
								<li><strong>Failure Handling:</strong> In case of job failures, details of the failed job status were written to Athena for further analysis.</li>
								<li><strong>Data Visualization:</strong> Looker was employed to visualize job failures through charts and donuts, with Athena as the database and automatic refresh set to 20 seconds.</li>
								<li><strong>Analyzing Job Failures:</strong> Looker dashboards provided insights into major reasons for job failures, facilitating corrective actions to succeed in resolving failures within the last 7 days. These dashboards were shared among the team for collaborative analysis.</li>
							</ul>
						</div>
					</header>
				</section>
			</div>
		</div>
	</body>
</html>
